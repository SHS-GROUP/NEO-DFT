                                                         August 10, 2011

This is the README file for Andrey Asadchev's "libcchem", a C++ library
for fast quantum chemistry on Nvidia GPUs, using CUDA software.

The libcchem toolkit was written in 2010 and 2011 by Andrey Asadchev
in the Department of Chemistry at Iowa State University.

Two functionalities are provided:
a) 2e- AO integrals and their assembly into a closed shell Fock matrix.
   This can be used with standard parts of GAMESS (mainly gradient codes)
   to do the rest of an SCF calculation, and to optimize RHF geometries.
   This can utilize all GPUs and CPUs present in a node, and can use more
   than  one such node (for example, in an Infiniband cluster).
b) the closed shell MP2 energy, at a single geometry.
   This can utilize all CPUs present, but only in one node.

GAMESS itself will read the basis set, atomic coordinates, do the 1e-
integrals, prepare the initial orbital guess, drive the SCF convergence,
and depending on the type of run, compute the nuclear gradient.  Only
the afore-mentioned very time consuming steps are handled by libcchem
threads.  Other parts of GAMESS will run in parallel using the usual
codes in GAMESS itself, using the process-level parallization provided
by GAMESS' DDI library (which is a message passing interface to MPI
or TCP/IP or so on).

Arithmetic in the GPU is done at double precision, and thus all results
are equally accurate to normal GAMESS runs.

The integral algorithm for the GPU is described in
    Uncontracted Rys Quadrature Implementation of up to G Functions
                 on Graphical Processing Units.
    Andrey Asadchev, Veerendra Allada, Jacob Felder, Brett M. Bode,
               Mark S. Gordon, Theresa L. Windus
     Journal of Chemical Theory and Computation 6, 696-704(2010).

------------------------------------------------------------------------

            To utilize the "libcchem" GPU code with GAMESS:

1. Obtain the quantum chemistry software:

libcchem and GAMESS and DDI are available in a single tar file from:
      http://www.msg.chem.iastate.edu/GAMESS/GAMESS.html
After registering for a properly licensed source code copy of GAMESS at
our web site, for 'linux64' systems, follow its normal installation
procedures: ~/gamess/misc/readme.unix.  This produces a version of
GAMESS that does -not- use the GPUs.  The directions below assume that
you compile, link, and execute GAMESS w/o the GPU code at this point,
before going on.


2. Obtain additional software:

     a) Nvidia's CUDA software, to prepare the GPU threads
        The version is revealed by typing "nvcc -V".
The CUDA compiler 'nvcc' must be on your path, e.g. /usr/local/cuda/bin,
The CUDA libraries will be needed when GAMESS is linked, and when the
program is run: e.g. your LD_LIBRARY_PATH should contain something
similar to /usr/local/cuda/lib64

     b) a c++ compiler, to prepare the CPU threads.
        The version is revealed by typing "g++ -v".
It is better to use g++ 4.3 than g++ 4.1, but both should work.
The latter cannot compile some of the OpenMP directives found in
the libcchem source codes.

     c) cheetah: www.cheetahtemplate.org
        The version is revealed by typing "cheetah".
This is a code preprocessor acting on the libcchem C++ source codes.
Your execution path must be able to locate the binary file 'cheetah',
during the 'make' steps that compile libcchem.  To install Cheetah
inside its own source tree,
   cd Cheetah-2.4.4
   python setup.py install --prefix ~/Cheetah-2.4.4 \
        --install-lib ~/Cheetah-2.4.4/lib

     d) Boost: www.boost.org
        The version is revealed by 'ls -lF /your/path/to/libboost_thread.so'
This is a set of libraries aimed at standardizing C++ codes.  The
configuration step below requires naming a path pointing to Boost's
include files, and Boost's thread library is used at link time.

Boost requires a few changes to use it with the 'nvcc' compiler
from Nvidia.  We provide three sets of these, for versions 1.43 and
two flavors of 1.46.  These patch files can be found in the
subdirectory ~/gamess/libcchem/boost.  For example, chdir into
the directory where you download Boost, and
    patch -p0 < /full/path/name/to/boost-1.46.1.nvcc.patch

     e) Hierarchical Data Format: http://www.hdfgroup.org
        The version is revealed by "h5ls --version".
This is an I/O library supporting parallel access.  When compiling the
HDF5 software, be sure to enable the C++ binding, and thread safe mode,
by choosing HDF5 configure options
  --with-pthread --enable-cxx --enable-threadsafe --enable-unsupported
HDF5 uses the zlib compression library (-lz), probably already found
on your computer, e.g. from a zlib-devel RPM: otherwise see www.zlib.net.
Depending on how HDF5 is built, -lsz may also be needed to find szip.

     f) have a typical 64-bit Linux: GNU compiler suite with their
libraries, or perhaps a licensed Intel FORTRAN compiler, some
mathematical library for BLAS such as Intel's MKL, C-shell (tcsh),
Python, make/automake/libtool, and so forth.  The "f" software is
sufficient to install the non-GPU GAMESS code, while "b" to "e" are
specifically required to use libcchem.

Caspar Steinman has written some notes about his experience
trying to use the first version of this software, which was
called 'libqc'.  It may help to read his experience,
     http://combichem.blogspot.com/2011/02/
  compiling-gamess-with-cuda-gpu-support.html
That's all supposed to be on one line, of course.


3. Compile the quantum chemistry library "libcchem"

Note that all commands below are C-shell syntax, adjust as needed
to any other shell's file redirections.

We tested this on two different Infiniband clusters, with two different
generations of GPU devices:
                     older              newer
       CPU=    E5450, 8 cores      X5650, 6 cores
       RAM=    16 GBytes/node      24 GBytes/node
       GPU=    Tesla T10 pairs     Fermi C2070 pairs or quads
       RAM=    4 GBytes/GPU        5.25 GBytes/GPU (ECC mode)
   FORTRAN=    gfortran 4.1.2      ifort 12.0.4.191   for GAMESS
 Intel MPI=         3.2.0              4.0.2.003      for DDI
      CUDA=         3.2                4.0            for libcchem
       g++=         4.1.2              4.1.2          for libcchem
   cheetah=         2.4.2.1            2.4.4          for libcchem
     Boost=         1.46.0             1.46.1         for libcchem
      hdf5=         1.6.10             1.8.7          for libcchem

Compiling is three steps.

First, the "libcchem" software must be configured before it can be
compiled.  Help for the configuration options is "configure --help".

Our newer Fermi boards used

    ./configure --prefix=/home/mike/gamess/libcchem \
                --with-gamess \
                --with-integer8 \
                --with-mkl \
                --with-cuda=/share/apps/cuda \
                --with-boost=/usr/local \
                --with-hdf5=/usr/local \
                --with-gpu=fermi \
                --disable-openmp \
                CXX='g++'  CXXFLAGS='-O3 -march=core2'   >& step1.log

while our older Tesla system needs to specify its different type
of GPU, as well as some minor path name changes:

   ./configure --prefix=/home/mike/gamess/libcchem \
               --with-gamess \
               --with-integer8 \
               --with-mkl \
               --with-cuda=/usr/local/cuda \
               --with-boost=/usr/local \
               --with-hdf5=/usr/local \
               --with-gpu=tesla \
               --disable-openmp \
               CXX='g++' CXXFLAGS='-O3 -march=core2'   >& step1.log

Note that our use of g++ 4.1 requires that OpenMP be turned off.

Obviously, the configure command is typed on one line in real life.
It will produce two output files named config.log and config.status,
in addition to the output step1.log.  CXX names the desired C++
compiler, and CXXFLAGS selects high optimization for both the CPU
code and the GPU code.  This step prepares 'Makefile' which should
contain your desired compiler options.

The final two steps are
     make         >& step2.log &
     make install >& step3.log
The 2nd step compiles 'libcchem', taking about 20 minutes on our system.
The 3rd step copies the libraries to a subdirectory "lib" located
beneath the --prefix path.  If this step finishes correctly, you
will find three libraries there,
    libcchem_gamess.a  - interfaces to GAMESS
    libcchem.a         - Fock construction
    librysq.a          - two electron integrals by Rys quadrature
along with their .la counterparts.


4. attaching "libcchem" to GAMESS/DDI

Since only a few GPU kernels exist, we only need to re-compile a few
files included in standard GAMESS, so that they invoke the "libcchem"
package, and then relink.  This presumes step 1 was finished!

a) vi comp, set GPUCODE=true  (i.e. edit where GPUCODE is set false)
b) ./comp rhfuhf            compiles in calls to the "libcchem" package
   ./comp mp2               compiles in calls to the "libcchem" package
c) vi lked, set GPUCODE=true  (i.e. edit where GPUCODE is set false)
   Also, change the library pathnames, at the second place the
   string 'GPUCODE' appears.  The pathnames must point to your
   libcchem, CUDA, Boost, and HDF5 libraries.
d) ./lked gamess gpu.00     produces a binary named gamess.gpu.00.x

Comments about the linking step:

"libcchem" produces three .a files for a static link, meaning that
the libraries only need to be on the head node, and end up included
inside the binary.  The Boost and HDF5 libraries are also linked
as static libraries, therefore, only your head node needs to have
these libraries.  However, the CUDA link is dynamic, against .so
libraries, so every compute node needs a CUDA runtime installation,
and as noted again just below, these CUDA .so libraries need to be
located at run time.

The command "ldd gamess.gpu.x" will show the full set of dynamically
linked libraries required to exist on your compute nodes.   The most
convenient way to locate them is by the LD_LIBRARY_PATH environment
variable.


5. running GAMESS with libcchem

The libcchem runs using threads, some running on the CPU(s), and
some running on the GPU(s).  Threads are somewhat contradictory with DDI,
which runs GAMESS as distinct processes.  So, during any computational
kernel handled by libcchem, only the first GAMESS/DDI process in a node
will generate GPU/CPU threads.  The other GAMESS/DDI processes will
just go to sleep until the libcchem step is finished, waiting to join
back in at later steps, such as the nuclear gradient.  By spawning
as many CPU threads during the Fock build as you have cores, and
as many GPU threads as you have GPUs, all computational devices will
continue to be utilized.  Note that there is a single supervisory
thread, so that the number of CPU threads doing quantum chemistry is
normally one less than the number requested.

The two libcchem kernels are
   a) closed shell Fock construction (including all AO integrals),
      this may be combined with standard GAMESS codes for nuclear
      gradient runs, such as geometry optimizations.
      This is always AO integral direct (DIRSCF is ignored, FDIFF is obeyed).
      Note that closed shell DFT is not enabled.
      More than one GPU/CPU node can be used.
A libcchem-linked binary ALWAYS carries out the RHF Fock builds in
the libcchem code.
   b) closed shell MP2 energy, at a single geometry.
      This is also always AO integral direct.
      Memory used by each thread will be NAO*MXSH*OCC*OCC/2, where NAO
      is the basis set size, MXSH is the largest shell's size (s=1,
      p=3, d=6, f=10, g=15), and OCC is the number of correlated
      orbitals (occupied).
      A temporary work file named 'data.h5' will be used, in the
      job's working directory, which should be a fast file system.
      Only one node can be used, and threads presently run only
      on the CPU cores.
A libcchem-linked binary, by default, will evaluate the MP2 energy
in the libcchem code, unless the $MP2 input specifically requests
a different algorithm than CODE=GPU.

To run GAMESS with "libcchem", edit the 'rungms' script.

At the top, select the execution target as 'sockets' or 'mpi'.
The processes are started according to the normal kickoff procedures
for the chosen DDI messaging mechanisms, e.g. ddikick.x or else
however your MPI starts up.

Since our two GPU clusters both run over MPI, the "libcchem" specific
stuff is found only in the MPI part of 'rungms'.  It should be easy
to copy this into the sockets part, if you need to.

As usual, several other customizations in 'rungms' are needed, to
point to the scratch working directory, locate the auxiliary data
files needed by GAMESS, as well as locating the GAMESS (and perhaps
ddikick.x) binary, and so forth.

In principle, you already did all this when installing GAMESS/DDI at
step 1 of these instructions.  Very few additional changes are
needed to use "libcchem", see below.

One new edit to 'rungms' that is almost certainly necessary is adding
the path to your CUDA run time libraries (the .so files mentioned
in the previous section), as an additional part of LD_LIBRARY_PATH.
Search for the string "NUMGPU" to find the right place in 'rungms'.

The same section of 'rungms' can set three environment variables,
although all three may be omitted:
    a) NUM_THREADS controls the number of CPU threads, and this
       variable should normally equal the number of cores in each
       node.  If not given, the default is to run on all cores.
       One CPU thread in each node will act as a supervisor, so one
       fewer than NUM_THREADS threads perform quantum chemistry.
    b) GPU_DEVICES is a list of GPUs to be used.  CUDA 4.0's
             ssh compute-0-10 'nvidia-smi -L'
       command will list the device numbers.  For example
             setenv GPU_DEVICES 0,1
       uses the first two GPUs present.
       The default is to use all available GPUs in the node.
       Set GPU_DEVICES to -1 to generate only CPU threads.
    c) CCHEM_PROFILE may be set to 1 to show detailed time
       consumption for CPU and GPU threads.

As already noted, one GAMESS/DDI process per node generates the
various "libcchem" threads, after which all GAMESS/DDI processes
sleep, until the libcchem kernel finishes execution.  The sleeping
processes will then awaken, and resume computing.

The first process will appear to consume very large amounts of CPU
time, as all time used by all CPU threads is charged to this process,
which started the threads.  The wall clock times in the output file
remain normal (correct).

                         * * * * *

In the absence of a batch manager, execute input file xxx.inp by
      rungms xxx gpu.00 ncpu >& xxx.log &
The 2nd parameter assumes that you named the binary 'gamess.gpu.00.x',
it is just the middle part of the binary's name.
The 3rd parameter passed to this script will control the number of
DDI processes generated (it is not related to the number of GPU
or CPU threads used by libcchem).

It is likely that a GPU cluster will have a proper batch queue manager.
Please see
    ~/gamess/misc/gms
for a script that uses PBS or SGE batch managers on different clusters
at our university, two of which have GPU boards present.


------------------------------------------------------------------------

                            Timing examples

"old" node, with Tesla GPU:
  hardware= Dell R5400n with 16x PCIe,
            two quad-core 3.0 GHz Intel E5450 (Harperton) processors,
            Mellanox MT25204 4x DDR Infiniband,
            16 GBytes RAM, i.e. 2 GB/CPU core,
            two striped SATA disks,
            quad-GPU Nvidia S1070 units connected to two nodes,
            so each test node has two "Tesla" T10 processors,
            each T10 with 4 GBytes memory.
  software= RedHat Enterprise Linux 2.6.18-92.el5
            gfortran 4.1, g++ 4.1.2, Intel's MKL 10.0,
            Intel MPI 3.2,
            CUDA nvcc 3.2
            cheetah 2.4.2.1, Boost 1.46.0, HDF5 1.6.10

"new" node, with Fermi GPU:
  hardware= Atipa node.
            one hex-core Intel 2.67 GHz X5650 (Gulftown) processor,
            Qlogic IBA7322 4x DDR Infiniband,
            24 GBytes RAM, i.e. 12 GB/CPU core,
            two striped 3 Gbps 7200 RPM SATA disks,
            some nodes have two and some have four Fermi C2070 cards,
            each C2070 with 5.25 GBytes of ECC memory.
  software= RedHat Enterprise Linux 2.6.18-194.17.4.el5
            ifort 12.0.4.191, g++ 4.1.2, Intel's MKL 12.0,
            Intel MPI 4.0.2,
            CUDA nvcc 4.0
            cheetah 2.4.4, Boost 1.46.1, HDF5 1.8.7

Timing examples are provided for closed shell SCF using Pople style
and correlation consistent basis sets.  Two closed shell MP2 timings
are also included.


a. vancomycin: C66 N9 O24 Cl2 H75, direct SCF converges in 15 iterations
there are 176 atoms, 380 occupied orbitals, 1673 AOs & MOs for 6-31G(d).

GAMESS+libcchem: E= -5748.5708774945 to ..40, 1,2,4,8 CPU threads, 2 GPU
                 E= -5748.5708774938 to ..40, 1,    8 CPU threads, 0 GPU
         GAMESS: E= -5748.5708774573 to ..74, 1,8 CPUs, best integral codes
         GAMESS: E= -5748.5708774579, using only rotated axis codes
         GAMESS: E= -5748.5708774558, using only Rys quadrature code

GAMESS only, old Tesla node, best integral algorithms
      total   guess     SCF   property
#CPU   wall     CPU     CPU     CPU
  1  21,366   211.3  20,988.9 153.1
  8   2,941   144.6   2,731.2  33.1
 16   2,628   111.4   2,171.6  20.9
Using p=1 and only Rys quadrature integrals takes 29,676 wall seconds.
Using p=1 and only rotated axis integrals   takes 23,470 wall seconds.

GAMESS+libcchem, old Tesla node, using both GPUs, one or two total nodes:
      total   first iteration   final iteration
#CPU   wall      GPU   CPU         GPU   CPU
  8   2,642     3:33  3:46        1:22  1:23
 16   2,565     2:34  3:10        1:00  1:29
Timing is for a typical GPU or CPU thread.  Note FDIFF in $SCF means
fewer integrals are computed in the final iterations than at the start.

GAMESS only, new Fermi node
      total   guess    SCF   property
#CPU   wall     CPU    CPU     CPU
  1  15,669    74.1 15,457.7 138.3
  6   2,908    61.7  2,767.2  29.1
 12   1,512    44.5  1,387.8  16.2
 24     814    37.5    702.1   9.6

GAMESS+libcchem, new Fermi node, using 0,2,4 GPUs and 1,2,4 total nodes:
                         first iteration  final iteration
#CPU  #GPU   total wall     GPU   CPU       GPU   CPU  (wall seconds)
  1     0     17,733        --  27:35       --  10:48
  6     0      3,303        --   4:53       --   1:54
 12     0      1,765        --   2:32       --   0:59
 24     0      1,042        --   1:24       --   0:33

  1     2      3,541       3:42  5:03      1:46  2:15
  6     2      2,257       2:58  3:03      1:19  1:21
 12     2      1,235       1:31  1:41      0:41  0:43
 24     2        766       0:45  0:57      0:21  0:26

  1     4      3,507       1:57  5:02      0:57  2:14
  6     4      1,741       2:10  2:15      1:03  1:05
 12     4        987       1:05  1:09      0:33  0:35
 24     4        643       0:36  0:51      0:17  0:22
This table records the longest GPU or CPU time of any node used.


b. ginkgolide A: C20 O9 H24, direct SCF converges in 13 iterations
there are 53 atoms, 108 occupied MOs, 1375 AOs and 1206 MOs for cc-pVTZ
The libcchem likes the f angular momenta basis functions now present.

GAMESS+libcchem: E= -1445.5558009259
    GAMESS, p=1: E= -1445.5558009249, using best integral codes
    GAMESS, p=8: E= -1445.5558009251, using best integral codes
    GAMESS, p=1: E= -1445.5558009247, using only Rys quadrature

GAMESS only, old Tesla node, best integral algorithms
      total   guess    SCF   property
#CPU   wall     CPU    CPU     CPU
  1  56,400    88.0  56215.3  86.7
  8   7,218    63.3   7109.5  28.3
 16   6,048    50.0   5783.3  20.0
Using p=1 and only Rys quadrature integrals takes 66,601 wall seconds.

GAMESS+libcchem, old Tesla node, using both GPUs, one or two total nodes:
      total   first iteration   final iteration
#CPU   wall      GPU   CPU         GPU   CPU
  8   5,226     9:40  9:49        3:04  3:05
 16   4,508     6:21  9:07        2:10  2:12

GAMESS only, new Fermi node
      total   guess     SCF   property
#CPU   wall     CPU     CPU     CPU
  1  46,958    25.3  46,856.5  74.5
  6   7,851    25.9   7,780.1  19.3
 12   3,966    20.2   3,900.7  11.8
 24   2,112    17.3   2,046.6   8.2

GAMESS+libcchem, new Fermi node, using 0,2,4 GPUs and 1,2,4 total nodes:
                         first iteration  final iteration
#CPU  #GPU   total wall     GPU   CPU       GPU   CPU (wall seconds)
  1     0     37,238        --  71:36       --  22:59
  6     0      6,582        --  12:30       --   4:01
 12     0      3,333        --   6:16       --   2:01
 24     0      1,710        --   3:09       --   1:01

  1     2     12,324      19:02 24:10      5:50  7:24
  6     2      3,833       7:03  7:08      2:20  2:22
 12     2      1,997       3:29  3:45      1:11  1:12
 24     2      1,091       1:46  2:01      0:36  0:37

  1     4     12,276      18:23 24:07      5:34  7:22
  6     4      3,690       6:14  7:00      1:52  2:13
 12     4      1,961       2:13  3:57      0:57  1:10
 24     4      1,302       1:02  2:22      0:24  0:45


c. absinthin: C30 O6 H40, using 6-31G(d), closed shell MP2 energy.
there are 76 atoms, 138 occupied MOs (102 correlated valence), 620 AOs & MOs

This is just at the molecular size limit for the distributed memory
MP2 energy program in GAMESS (CODE=DDI in $MP2) to fit into the
memory of a single 16 GByte test nodes: MWORDS=10, MEMDDI=1900.
The other option in GAMESS (CODE=IMS) stores data on disk.

These two may be compared to the "libcchem" implementation, CODE=GPU,
which is the default when GAMESS is linked to "libcchem".  It too
stores data on disk, through the HDF5 library.

SCF step timings are typical of the 1st example above, as the basis
set tops out at d functions.

old Tesla node:
  code=ddi: SCF=311, MP2=2611, total CPU=2928, total wall= 3047  (p=8)
  code=ims: SCF=309, MP2= 622, total CPU= 937, total wall= 1276  (p=8)
  code=gpu: SCF=333(wall), MP2=821(wall),      total wall= 1156  (p=8 + 2 GPU)

new Fermi node:
  code=ddi: SCF=321, MP2=1685, total CPU=2012, total wall= 2040  (p=6)
  code=ims: SCF=322, MP2= 463, total CPU= 790, total wall=  803  (p=6)
  code=gpu: SCF=189(wall), MP2=713(wall),      total wall=  904  (p=6 + 4 GPU)


d. capreomycin 1B: C25 H44 O7 N14, using 6-31G(d), closed shell MP2 energy.
there are 90 atoms, 174 occupied MOs (128 correlated valence), 778 AOs & MOs

This example is now too large for the distributed memory MP2 program
to fit into one node: MWORDS=12, MEMDDI=5050, so the DDI run is on
four 16 GByte nodes or three 24 GByte nodes, connected by Infiniband,
using Intel's MPI.  This MP2 program is a little bit network bound due
to messaging (CPU < wall).

On the other hand, the timing for libcchem is from just a single node!

old Tesla node:
   code=ddi: SCF= 126(CPU),  MP2=1082(CPU),     total  CPU=1186,
             SCF= 142(wall), MP2=4723(wall),    total wall=4587  (4 nodes, p=32)
   code=ims: SCF= 356(CPU),  MP2=1202(CPU),     total  CPU=1570,
             SCF= 368(wall), MP2=6137(wall),    total wall=6506  (1 node, p=8)
   code=gpu: SCF= 374(wall), MP2=2311(wall),    total wall=2688  (1 node+2 GPU)

new Fermi node:
   code=ddi: SCF= 105(CPU),  MP2=1070(CPU),     total  CPU=1215,
             SCF= 133(wall), MP2=2791(wall),    total wall=2936  (3 nodes, p=18)
   code=ims: SCF= 369(CPU),  MP2= 810(CPU),     total  CPU=1188,
             SCF= 380(wall), MP2=1069(wall),    total wall=1452  (1 node, p=6)
   code=gpu: SCF= 463(wall), MP2=1893(wall),    total wall=2359  (1 node+0 GPU)
             SCF= 293(wall), MP2=1930(wall),    total wall=2226  (1 node+2 GPU)
             SCF= 230(wall), MP2=1890(wall),    total wall=2124  (1 node+4 GPU)

Agreement of results:
  GAMESS: E(RHF)= -2258.7646954875, 15 iters, E(MP2)= -2265.4580532991 (DDI)
  GAMESS: E(RHF)= -2258.7646954875, 15 iters, E(MP2)= -2265.4580533006 (IMS)
libcchem: E(RHF)= -2258.7646955001, 15 iters, E(MP2)= -2265.4580533114 (GPU)

