#!/bin/csh
#
#    March 2014.
#    Build and execute the DDI test program, using pre-existing libddi.a,
#    then erase the binary of the test program when done.
#    This script can cope with either TCP/IP and MPI communications.
#
#    You will need to pay a little attention to the FORTRAN choice,
#    and maybe to linking, and to execution (including host names):
#          This is set up for use at Iowa State!
#    It should not be hard to put in your site's host names, and so forth.
#
#    Of course, your FORTRAN and communication library choice here
#    must exactly match how your 'compddi' step pre-built your libddi.a!
#
set COMMTYPE=sockets
#
echo Building the test program...
if (-e ../libddi.a) then
else
   echo "You must use 'compddi' in the DDI directory to build the"
   echo "DDI library 'libddi.a' before running this script."
   exit 4
endif
#
switch ($COMMTYPE)
#
case sockets:
#
#    linux64, using ifort
ifort -O2 -i8 -o ddi_test.x ddi_test.f -L../ -lddi -lpthread
#
#    linux64, using gfortran
#--gfortran -O2 -m64 -fdefault-integer-8 -std=legacy -o ddi_test.x \
#--         ddi_test.f -L../ -lddi -lpthread
#
#    ibm64
#--xlf -q64 -qintsize=8 -o ddi_test.x ddi_test.f -L../ -lddi -lpthread
#    axp64
#--f77 -i8 -v -o ddi_test.x ddi_test.f -L../ -lddi -lpthread
#    hpux64
#--f90 +DD64 +O2 +i8 +noppu -o ddi_test.x ddi_test.f -L../ -lddi \
#--        -ldld +U77 -lpthread
#    mac32
#--gfortran -O2 -std=legacy -o ddi_test.x ddi_test.f -L../ -lddi -lpthread
#    mac64
#--gfortran -O2 -m64 -fdefault-integer-8 -std=legacy -o ddi_test.x \
#-          ddi_test.f -L../ -lddi -lpthread
#    sun32
#--f77 -fast -O4 -xarch=v8plus            -o ddi_test.x ddi_test.f \
#--           -L../ -lddi -lsocket -lnsl -lposix4 -lpthread
#    sun64 on Sparc
#--f90 -xarch=v9 -O2 -xtypemap=integer:64 -o ddi_test.x ddi_test.f \
#--           -L../ -lddi -lsocket -lnsl -lrt     -lpthread
#    sun64 on Opteron
#--f90 -m64 -O2 -xtypemap=integer:64      -o ddi_test.x ddi_test.f \
#--           -L../ -lddi -lsocket -lnsl -lrt     -lpthread
#
#      TCP/IP sockets execution, with kickoff program ddikick.x
#
#      You need to choose between old-fashioned rsh or the more
#      secure 'ssh' remote command launcher.  The former requires
#      setting up .rhosts files, the latter requires ssh keys exist.
#
#      the test program binary must be in the same directory as the
#      DDI kickoff program, built at the same time as libddi.a.
#      Edit DDIPATH to where your kickoff program is located.
#
#      below are a lot of host names used at Iowa State University,
#      undoubtedly the 'elements' of your cluster are different.
#
if (-x ddi_test.x) then
   echo Executing the test program...
   #      select remote shell from ssh, rsh, remsh(HP-UX's rsh)
   setenv DDI_RSH rsh
   set DDIPATH=/u1/mike/gamess/ddi
   mv ddi_test.x $DDIPATH
   $DDIPATH/ddikick.x $DDIPATH/ddi_test.x dummyarg \
      -ddi 4  8 si:cpus=2 pd:cpus=2 pt:cpus=2 ag:cpus=2 -scr /tmp
#
#             some other examples of host names follow:
#          the first integer is number of unique computer names to follow
#          the second integer is sum of all :cpus values
#          the :cpus tells how many cores to use in each host.
#     -ddi 1  4 br:cpus=4                               -scr /tmp
#     -ddi 2  4 se:cpus=2 sb:cpus=2                     -scr /tmp
#     -ddi 2  4 zr:cpus=2 nb:cpus=2                     -scr /tmp
#     -ddi 1  2 te:cpus=2                               -scr /tmp
#     -ddi 1  4 as:cpus=4                               -scr /tmp
#     -ddi 2  4 hf:cpus=2 ta:cpus=2                     -scr /tmp
#     -ddi 1  2 ti:cpus=2                               -scr /tmp
#     -ddi 2  4 in:cpus=2 sn:cpus=2                     -scr /tmp
#
   rm $DDIPATH/ddi_test.x
else
   echo oops, the build of the test program for TCP/IP sockets had a booboo.
endif
breaksw

case mpi:
#
#    linux64: compile using ifort, link against Intel's iMPI
#
#              ISU's cluster 'dynamo' has a very old iMPI library (3steps)
ifort -O2 -i8 -vec-report0 -o ddi_test.x ddi_test.f -L../ -lddi \
      -L/opt/intel/impi/3.2/lib64 -lmpi -lmpigf -lmpigi -lrt \
      -lpthread
#
#              ISU's cluster 'exalted' has a more modern iMPI library (hydra)
#--ifort -O2 -i8 -vec-report0 -o ddi_test.x ddi_test.f -L../ -lddi \
#--      -L/share/apps/intel/impi/4.0.2.003/lib64 -lmpi -lmpigf -lmpigi -lrt \
#--      -lpthread
#
#    linux64: compile using ifort, link against MVAPICH2 and OFED
#
#              ISU's cluster 'dynamo' using MVAPICH2 (3steps kickoff)
#
#--ifort -O2 -i8 -vec-report0 -o ddi_test.x ddi_test.f -L../ -lddi \
#--      -L/share/apps/mpi/mvapich2-1.9a2-sock/lib -lmpich -lopa -lmpl \
#--      -L/usr/lib64 -libverbs -lrdmacm -libumad -lrt \
#--      -lpthread
#
#      execution, with your particular MPI's kickoff procedures.
#
#      User choices here:
#       1. set total processor cores (NPROCS) and nodes (NNODES) to use.
#          It is up to -YOU- to ensure NNODES divides evenly into NPROCS!
#       2. kickoff style choice (3steps/hydra).
#       3. ISU-specific host names are used in the host/process files!
#          Provide NNODES total hostnames in the configuration files below.
#          ISU head nodes named dynamo/exalted have Infiniband cards,
#          so at ISU we don't have to log into compute nodes 1st.
#
if (-x ddi_test.x) then
   set NPROCS=4
   set NNODES=2
   set MPIKICK=3steps
   #
   @ PPN = $NPROCS / $NNODES
   @ PPN2 = $PPN + $PPN
   echo Running $PPN compute processes per node, as well as $PPN data servers.
   #
   #        we will show both the old '3steps' and the modern 'hydra'
   switch ($MPIKICK)
      case hydra:
         #         uses one config file containing hostnames.
         setenv PROCFILE ddi_test.procs
         echo "exalted:$PPN2"     >> $PROCFILE
         echo "compute-0-1:$PPN2" >> $PROCFILE
         #
         setenv I_MPI_HYDRA_ENV all
         setenv I_MPI_PERHOST $PPN2
         @ NPROCS2 = $NPROCS + $NPROCS
         mpiexec.hydra -f $PROCFILE -n $NPROCS2 ddi_test.x < /dev/null
         #
         rm ddi_test.x
         rm $PROCFILE
         unsetenv $PROCFILE
         breaksw
      case 3steps:
         #         uses two different config files containing hostnames.
         setenv HOSTFILE ddi_test.hosts
         echo "dynamo"      >> $HOSTFILE
         echo "compute-0-0" >> $HOSTFILE
         #
         setenv PROCFILE ddi_test.procs
         echo "-n $PPN2 -host dynamo      ddi_test.x" >> $PROCFILE
         echo "-n $PPN2 -host compute-0-0 ddi_test.x" >> $PROCFILE
         #
         set echo
         mpdboot --rsh=ssh -n $NNODES -f $HOSTFILE
         mpiexec -configfile $PROCFILE < /dev/null
         mpdallexit
         unset echo
         #
         rm ddi_test.x
         rm $HOSTFILE
         rm $PROCFILE
         unsetenv $HOSTFILE
         unsetenv $PROCFILE
         breaksw
      default:
         breaksw
   endsw
else
   echo oops, the build of the test program for MPI had a booboo.
endif
breaksw

   default:
      echo unknown communication mode chosen
      breaksw
endsw
#
exit
